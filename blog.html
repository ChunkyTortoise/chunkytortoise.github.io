<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Blog | Cayman Roden</title>
    <meta name="description" content="Technical blog on LLM cost engineering, multi-agent orchestration, and RAG pipeline optimization. Real metrics from production AI systems.">
    <meta property="og:title" content="Blog | Cayman Roden">
    <meta property="og:description" content="Technical blog on LLM cost engineering, multi-agent orchestration, and RAG pipelines. Real metrics from production systems.">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://chunkytortoise.github.io/blog.html">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Blog | Cayman Roden">
    <meta name="twitter:description" content="Technical blog on LLM cost engineering, multi-agent orchestration, and RAG pipelines.">
    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="style.css">
</head>
<body class="bg-gray-50 text-gray-900">
    <nav class="bg-white border-b border-gray-200 sticky top-0 z-50">
        <div class="max-w-6xl mx-auto px-4 py-3 flex justify-between items-center">
            <a href="index.html" class="text-lg font-bold text-indigo-600">Cayman Roden</a>
            <div class="flex gap-4 text-sm">
                <a href="projects.html" class="hover:text-indigo-600">Projects</a>
                <a href="services.html" class="hover:text-indigo-600">Services</a>
                <a href="certifications.html" class="hover:text-indigo-600">Certifications</a>
                <a href="case-studies.html" class="hover:text-indigo-600">Case Studies</a>
                <a href="benchmarks.html" class="hover:text-indigo-600">Benchmarks</a>
                <a href="blog.html" class="text-indigo-600 font-medium">Blog</a>
                <a href="about.html" class="hover:text-indigo-600">About</a>
            </div>
        </div>
    </nav>

    <section class="max-w-4xl mx-auto px-4 py-12">
        <h1 class="text-3xl font-bold mb-2">Blog</h1>
        <p class="text-gray-600 mb-10">Technical writeups from building production AI systems.</p>

        <!-- Post 3 -->
        <article class="bg-white rounded-xl border p-8 mb-8">
            <div class="flex items-center gap-3 mb-4">
                <span class="bg-indigo-100 text-indigo-700 text-xs px-3 py-1 rounded-full font-medium">Multi-Agent</span>
                <span class="bg-purple-100 text-purple-700 text-xs px-3 py-1 rounded-full font-medium">Architecture</span>
                <span class="text-sm text-gray-400">February 8, 2026</span>
                <span class="text-sm text-gray-400">&middot; 6 min read</span>
            </div>
            <h2 class="text-2xl font-bold mb-6">Multi-Agent Orchestration: Building Reliable Cross-Bot Handoffs</h2>

            <div class="prose prose-gray max-w-none text-gray-700 space-y-4">
                <p>Everyone focuses on making individual AI bots smarter. Better prompts, finer-tuned intent detection, richer context windows. That work matters &mdash; but when you run multiple specialized bots in production, the hardest engineering problem is not inside any single bot. It is the moment one bot decides a conversation belongs to another bot and tries to hand it off mid-flow. That transition is where conversations get lost, users get confused, and your system's reliability collapses.</p>

                <p>I built a handoff service for <a href="https://github.com/ChunkyTortoise/EnterpriseHub" class="text-indigo-600 hover:underline" target="_blank">EnterpriseHub</a>, a real estate AI platform with three specialized chatbots: a Lead Bot that qualifies inbound contacts, a Buyer Bot that handles property searches and financing questions, and a Seller Bot that manages CMAs, listing prep, and home valuations. Each bot has its own personality, its own prompt engineering, and its own intent decoder. The challenge is that a lead who starts as a general inquiry might say "I want to buy a house with a $400K budget" halfway through the conversation &mdash; and the system needs to move that conversation from Lead Bot to Buyer Bot without losing context, without ping-ponging, and without triggering on noise.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">The Edge Cases That Break Naive Handoffs</h3>
                <p>A first-pass handoff system is straightforward: detect buyer or seller intent, switch bots. In testing, three failure modes showed up immediately:</p>

                <ul class="list-disc pl-6 space-y-2">
                    <li><strong>Circular handoffs.</strong> Lead Bot detects buyer intent, hands off to Buyer Bot. Buyer Bot sees the contact still has seller-related tags and hands back to Lead Bot. Lead Bot re-detects buyer intent. The contact bounces endlessly between bots, generating a stream of CRM tag changes and confusing the human agent monitoring the dashboard.</li>
                    <li><strong>Concurrent conflicts.</strong> Two webhook events fire within milliseconds of each other &mdash; one from a chatbot response, one from a CRM workflow trigger. Both attempt a handoff for the same contact simultaneously. Without coordination, the system applies contradictory tag changes, leaving the contact in an undefined state that neither bot recognizes.</li>
                    <li><strong>False-positive intent signals.</strong> A lead says "My sister just sold her home and she recommended you." The word "sold" combined with "home" triggers the seller intent regex. The lead is not a seller. The handoff is wrong, and recovering from a bad handoff is harder than never making one &mdash; because the context window in the receiving bot starts fresh, losing the qualifying conversation so far.</li>
                </ul>

                <h3 class="text-xl font-bold mt-8 mb-3">Solution 1: Circular Prevention with Temporal Windows</h3>
                <p>The handoff service maintains a per-contact history of every handoff that has occurred. Before executing any handoff, it checks two conditions:</p>

                <ol class="list-decimal pl-6 space-y-2">
                    <li><strong>Direct circular check:</strong> Has this exact source-to-target handoff already happened for this contact within the last 30 minutes? If Lead already handed to Buyer 10 minutes ago, a second Lead-to-Buyer handoff is blocked.</li>
                    <li><strong>Chain cycle detection:</strong> Does the proposed handoff complete a loop in the handoff chain? If the chain is Lead &rarr; Buyer &rarr; Seller and the proposed handoff is Seller &rarr; Lead, the system detects the cycle and blocks it.</li>
                </ol>

                <div class="bg-gray-50 rounded-lg p-4 my-4">
                    <p class="text-sm font-mono text-gray-600">
                        CIRCULAR_WINDOW_SECONDS = 30 * 60  # 30-minute lookback<br><br>
                        def _check_circular_prevention(cls, contact_id, source_bot, target_bot):<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;now = time.time()<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;cutoff = now - cls.CIRCULAR_WINDOW_SECONDS<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;history = cls._handoff_history.get(contact_id, [])<br><br>
                        &nbsp;&nbsp;&nbsp;&nbsp;# Check 1: Same source->target within window<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;for entry in history:<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if entry["from"] == source_bot and entry["to"] == target_bot<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and entry["timestamp"] > cutoff:<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return (True, "blocked within 30-min window")<br><br>
                        &nbsp;&nbsp;&nbsp;&nbsp;# Check 2: Chain cycle detection<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;chain = [e["to"] for e in reversed(history) if e["timestamp"] > cutoff]<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;if target_bot in chain:<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return (True, "would create cycle in handoff chain")
                    </p>
                </div>

                <p>The 30-minute window is deliberate. Too short (5 minutes) and a fast-talking lead can still trigger a loop. Too long (hours) and you block legitimate re-routes &mdash; a contact might genuinely shift from buyer mode to seller mode over the course of a longer conversation. Thirty minutes covers the realistic conversation cycle without being overly restrictive.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Solution 2: Rate Limiting per Contact</h3>
                <p>Even with circular prevention, a pathological pattern can generate excessive handoffs: a confused contact who alternates between buyer and seller language, or a bad actor probing the system. The service enforces two rate limits:</p>

                <ul class="list-disc pl-6 space-y-2">
                    <li><strong>3 handoffs per hour</strong> per contact. No legitimate conversation needs more than 3 bot transitions in 60 minutes. If this limit is hit, the contact stays with the current bot until the window clears.</li>
                    <li><strong>10 handoffs per day</strong> per contact. This catches slower-burn abuse or stuck automation loops that fire once every few minutes for hours.</li>
                </ul>

                <p>Both limits are checked before every handoff execution. When a rate limit blocks a handoff, the event is recorded in analytics so ops teams can investigate.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Solution 3: Contact-Level Locking for Concurrent Conflicts</h3>
                <p>When two handoff requests arrive for the same contact within milliseconds, the system needs to serialize them. The service uses a lightweight in-memory lock with a 30-second timeout:</p>

                <div class="bg-gray-50 rounded-lg p-4 my-4">
                    <p class="text-sm font-mono text-gray-600">
                        HANDOFF_LOCK_TIMEOUT = 30  # seconds<br><br>
                        def _acquire_handoff_lock(cls, contact_id):<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;now = time.time()<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;if contact_id in cls._active_handoffs:<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock_time = cls._active_handoffs[contact_id]<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if now - lock_time < cls.HANDOFF_LOCK_TIMEOUT:<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return False  # Another handoff in progress<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;cls._active_handoffs[contact_id] = now<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;return True
                    </p>
                </div>

                <p>The lock is acquired at the start of <code>execute_handoff()</code> and released in a <code>finally</code> block, ensuring cleanup even if the handoff fails. The 30-second timeout acts as a safety valve &mdash; if the lock holder crashes, the lock auto-expires rather than permanently blocking the contact.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">The Confidence Threshold System</h3>
                <p>Not all handoff directions are equally risky. Handing a lead to the wrong buyer specialist wastes expensive buyer-bot context. Handing a seller inquiry to the buyer bot could lose a listing. The system uses asymmetric confidence thresholds tuned to the cost of each mistake:</p>

                <div class="bg-gray-50 rounded-lg p-4 my-4">
                    <p class="text-sm font-mono text-gray-600">
                        # Confidence thresholds per handoff direction<br>
                        THRESHOLDS = {<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;("lead", "buyer"):&nbsp;&nbsp; 0.7,&nbsp; # Standard: clear buyer signals needed<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;("lead", "seller"):&nbsp; 0.7,&nbsp; # Standard: clear seller signals needed<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;("buyer", "seller"): 0.8,&nbsp; # High bar: buyer already in pipeline<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;("seller", "buyer"): 0.6,&nbsp; # Lower bar: sellers often also buy<br>
                        }
                    </p>
                </div>

                <p>The reasoning: buyer-to-seller handoffs get a 0.8 threshold because a contact already in the buyer pipeline has real momentum &mdash; they are looking at properties, discussing financing, building rapport with the buyer bot. Incorrectly rerouting them to the seller bot disrupts that flow. Conversely, seller-to-buyer gets a 0.6 threshold because sellers frequently also need to buy their next home. The "sell first, then buy" pattern is so common in residential real estate that a lower bar is justified.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Solution 4: Pattern Learning with Dynamic Thresholds</h3>
                <p>Static thresholds are a starting point, not an endpoint. The system records the outcome of every handoff &mdash; successful, failed, reverted, or timed out &mdash; and uses that data to adjust thresholds dynamically:</p>

                <div class="bg-gray-50 rounded-lg p-4 my-4">
                    <p class="text-sm font-mono text-gray-600">
                        MIN_LEARNING_SAMPLES = 10  # Don't adjust until enough data<br><br>
                        def get_learned_adjustments(cls, source_bot, target_bot):<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;outcomes = cls._handoff_outcomes.get(f"{source_bot}->{target_bot}", [])<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;if len(outcomes) < cls.MIN_LEARNING_SAMPLES:<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return {"adjustment": 0.0}  # Not enough data<br><br>
                        &nbsp;&nbsp;&nbsp;&nbsp;success_rate = sum(1 for o in outcomes<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if o["outcome"] == "successful") / len(outcomes)<br><br>
                        &nbsp;&nbsp;&nbsp;&nbsp;if success_rate > 0.8:&nbsp;&nbsp; return {"adjustment": -0.05}  # Lower bar<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;if success_rate < 0.5:&nbsp;&nbsp; return {"adjustment": +0.10}  # Raise bar<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;return {"adjustment": 0.0}  # Keep current threshold
                    </p>
                </div>

                <p>The minimum sample size of 10 prevents over-fitting to early data. The adjustment magnitudes are intentionally conservative: -0.05 for high success, +0.10 for low success. The asymmetry reflects that raising the bar (being more cautious) is safer than lowering it. Over time, each handoff route converges to its natural threshold based on real conversion data rather than guesses.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Intent Signal Extraction: Two Layers Deep</h3>
                <p>The handoff decision is only as good as the intent signals feeding it. The system extracts signals at two levels:</p>

                <ul class="list-disc pl-6 space-y-2">
                    <li><strong>Single-message analysis:</strong> Regex patterns match buyer phrases ("I want to buy," "budget $X," "pre-approval," "FHA," "VA loan") and seller phrases ("sell my house," "what's my home worth," "CMA," "listing my property"). Each pattern match adds 0.3 to the intent score, capped at 1.0.</li>
                    <li><strong>Conversation history scan:</strong> The last 5 messages are analyzed for the same patterns. History signals are blended at 50% weight: <code>buyer_score = min(1.0, current_score + history_signal * 0.5)</code>. This prevents a single ambiguous message from triggering a handoff while rewarding sustained intent across multiple turns.</li>
                </ul>

                <p>The blending ratio matters. At 100%, history would dominate and stale signals would persist too long. At 0%, only the latest message counts and the system loses the ability to detect gradually building intent. The 50% blend captures the pattern where a lead mentions buying once, then again two messages later &mdash; each mention individually might not cross the threshold, but together they do.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Results</h3>
                <div class="grid grid-cols-3 gap-4 my-6">
                    <div class="bg-green-50 rounded-lg p-4 text-center">
                        <p class="text-2xl font-bold text-green-700">0</p>
                        <p class="text-xs text-green-600 mt-1">Circular handoffs in production</p>
                    </div>
                    <div class="bg-green-50 rounded-lg p-4 text-center">
                        <p class="text-2xl font-bold text-green-700">12</p>
                        <p class="text-xs text-green-600 mt-1">False positives caught (week 1)</p>
                    </div>
                    <div class="bg-green-50 rounded-lg p-4 text-center">
                        <p class="text-2xl font-bold text-green-700">4 layers</p>
                        <p class="text-xs text-green-600 mt-1">Safety checks per handoff</p>
                    </div>
                </div>

                <p>The 12 false positives in the first week were almost all from the same pattern: leads mentioning selling in the context of a relative's experience rather than their own intent. Adding the conversation history scan and the 0.7 confidence threshold together filter these out &mdash; a single passing mention of "sold" scores around 0.3, well below the threshold, while genuine seller intent across multiple turns reliably crosses it.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Limitations and Tradeoffs</h3>
                <ul class="list-disc pl-6 space-y-2">
                    <li><strong>In-memory state is single-process.</strong> The handoff history, locks, and analytics live in class-level dicts. This works for a single-server deployment but would need Redis-backed state for horizontal scaling. The tradeoff is simplicity and zero-latency lookups versus multi-instance support.</li>
                    <li><strong>Regex intent detection has a ceiling.</strong> Pattern matching catches explicit phrases but misses implicit intent. A lead who says "We're outgrowing our current place and exploring options" is likely a buyer, but no regex pattern matches. Moving to LLM-based intent classification would improve recall at the cost of latency and token spend.</li>
                    <li><strong>The 30-minute circular window is domain-specific.</strong> Real estate conversations move at a particular pace. A customer support bot handling rapid-fire technical issues might need a 5-minute window. An insurance bot with multi-day workflows might need 24 hours. The constant is easy to change, but there is no auto-tuning for it.</li>
                    <li><strong>Pattern learning requires outcome data.</strong> The dynamic threshold adjustment only kicks in after 10 data points per route. In a new deployment, you are running on static thresholds for the first several days. If those thresholds are wrong, you accumulate bad handoffs before the system self-corrects.</li>
                </ul>

                <h3 class="text-xl font-bold mt-8 mb-3">The Key Lesson</h3>
                <p>The hard part of multi-agent AI is not building individual bots. Each of the three Jorge bots is a straightforward system: a prompt template, an intent decoder, a set of CRM integrations. The hard part is the transitions. When Bot A hands to Bot B, you are transferring not just a conversation thread but an entire context of qualification state, rapport, and user expectations. Getting this wrong is worse than never doing it &mdash; a misrouted contact is more frustrated than one who waited in the wrong queue.</p>

                <p>Every production multi-agent system I have seen underestimates this. The bot logic gets 90% of the engineering attention, and the handoff logic gets 10%. In practice, the ratio should be closer to 60/40. The handoff service in EnterpriseHub is ~350 lines of Python with no AI calls at all &mdash; it is pure control flow, state management, and safety checks. That unglamorous code is what makes the system reliable.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Try It Yourself</h3>
                <p>The full implementation is open source. The relevant files:</p>
                <ul class="list-disc pl-6 space-y-1">
                    <li><a href="https://github.com/ChunkyTortoise/EnterpriseHub/blob/main/ghl_real_estate_ai/services/jorge/jorge_handoff_service.py" class="text-indigo-600 hover:underline" target="_blank"><code>services/jorge/jorge_handoff_service.py</code></a> &mdash; Circular prevention, rate limiting, contact locking, pattern learning</li>
                    <li><a href="https://github.com/ChunkyTortoise/EnterpriseHub/blob/main/ghl_real_estate_ai/services/jorge/bot_metrics_collector.py" class="text-indigo-600 hover:underline" target="_blank"><code>services/jorge/bot_metrics_collector.py</code></a> &mdash; Per-bot stats, cache hits, alerting integration</li>
                    <li><a href="https://github.com/ChunkyTortoise/EnterpriseHub/blob/main/ghl_real_estate_ai/services/jorge/alerting_service.py" class="text-indigo-600 hover:underline" target="_blank"><code>services/jorge/alerting_service.py</code></a> &mdash; Configurable alert rules with cooldown periods</li>
                    <li><a href="https://github.com/ChunkyTortoise/EnterpriseHub/blob/main/ghl_real_estate_ai/services/jorge/ab_testing_service.py" class="text-indigo-600 hover:underline" target="_blank"><code>services/jorge/ab_testing_service.py</code></a> &mdash; A/B experiment management for handoff threshold tuning</li>
                </ul>

                <p class="mt-6">For the full performance metrics, see the <a href="benchmarks.html" class="text-indigo-600 hover:underline">benchmarks page</a>.</p>
            </div>
        </article>

        <!-- Post 2 -->
        <article class="bg-white rounded-xl border p-8 mb-8">
            <div class="flex items-center gap-3 mb-4">
                <span class="bg-indigo-100 text-indigo-700 text-xs px-3 py-1 rounded-full font-medium">RAG</span>
                <span class="text-sm text-gray-400">February 2026</span>
            </div>
            <h2 class="text-2xl font-bold mb-6">Why Single-Index RAG Fails and How Hybrid Retrieval Fixes It</h2>

            <div class="prose prose-gray max-w-none text-gray-700 space-y-4">
                <p>Most RAG tutorials show a simple pipeline: chunk documents, embed them, do a cosine similarity search, pass the top results to the LLM. It works for demos. It breaks in production. The problem is that a single retrieval method has systematic blind spots &mdash; dense embeddings miss exact keyword matches, and keyword search misses semantic paraphrases. I built a hybrid retrieval system that combines both, and it finds relevant documents that neither method catches alone.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">The Problem: Every Index Has Blind Spots</h3>
                <p>Consider a document about "Section 8 housing voucher programs." A user asks: "What government rental assistance programs are available?" Dense embeddings will match the semantic meaning &mdash; "government rental assistance" is conceptually similar to "housing voucher programs." But if the user asks "Section 8 requirements," a keyword search finds it instantly while the dense index may rank it lower because "Section 8" is a proper noun with no direct semantic relationship to generic terms.</p>

                <p>This is the fundamental tradeoff. Dense retrieval captures <em>meaning</em>. Keyword retrieval captures <em>specifics</em>. Production queries need both. The question is how to combine them without the scores from one method drowning out the other.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Architecture: Dual-Index with Reciprocal Rank Fusion</h3>
                <p>The system maintains two parallel indices over the same document chunks:</p>

                <ul class="list-disc pl-6 space-y-2">
                    <li><strong>BM25 (Okapi)</strong> &mdash; The keyword index. Uses term frequency with saturation (k1=1.5) and document length normalization (b=0.75). IDF is calculated as <code>log((N - df + 0.5) / (df + 0.5) + 1.0)</code> to prevent zero scores on common terms. This catches exact matches, proper nouns, and technical terminology.</li>
                    <li><strong>TF-IDF Dense Vectors</strong> &mdash; The semantic index. Scikit-learn's <code>TfidfVectorizer</code> with a vocabulary cap of 5,000 features generates dense vectors. Cosine similarity with L2 normalization handles the ranking. This catches paraphrases, synonyms, and conceptual similarity.</li>
                </ul>

                <p>Both indices return ranked results for every query. The challenge is combining them. You can't simply average the scores &mdash; BM25 scores and cosine similarities are on completely different scales with different distributions. A BM25 score of 12.7 and a cosine similarity of 0.83 aren't comparable.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Reciprocal Rank Fusion: The Key Insight</h3>
                <p>Instead of comparing raw scores, Reciprocal Rank Fusion (RRF) compares <em>positions</em>. Each result gets a score based on where it appears in each ranked list:</p>

                <div class="bg-gray-50 rounded-lg p-4 my-4">
                    <p class="text-sm font-mono text-gray-600">
                        # Reciprocal Rank Fusion<br>
                        # k=60 balances early-rank sensitivity<br><br>
                        def reciprocal_rank_fusion(ranked_lists, k=60, top_k=5):<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;scores = {}<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;for ranked_list in ranked_lists:<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for rank, (chunk_id, _) in enumerate(ranked_list):<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scores[chunk_id] = scores.get(chunk_id, 0)<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scores[chunk_id] += 1.0 / (k + rank)<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;return sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
                    </p>
                </div>

                <p>The k=60 constant controls how much an early rank matters versus a late rank. A document ranked #1 in one index gets a score of 1/61 = 0.0164. Ranked #10, it gets 1/70 = 0.0143. The falloff is gentle &mdash; being ranked 10th is only slightly worse than 1st. This means a document that ranks well in <em>both</em> indices reliably outscores a document that ranks #1 in one index but doesn't appear in the other.</p>

                <p>One implementation detail that matters: each index retrieves 2x the requested number of results before fusion. If you want the top 5, each index returns 10. This ensures that a document ranked #8 in one index and #3 in the other still gets considered, rather than being cut off.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Chunking: Sentence Boundaries Over Fixed Splits</h3>
                <p>Chunk quality directly determines retrieval quality. I use 500-character chunks with 50-character overlap and sentence-aware boundaries. The algorithm tries to break at sentence-ending periods first, then paragraph breaks, then line breaks, then spaces. It never breaks below half the chunk size to avoid creating fragments.</p>

                <div class="bg-gray-50 rounded-lg p-4 my-4">
                    <p class="text-sm font-mono text-gray-600">
                        # Smart boundary detection (priority order)<br>
                        separators = [". ", "\n\n", "\n", " "]<br>
                        min_break = chunk_size // 2  # 250 chars minimum<br><br>
                        for sep in separators:<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;break_point = text.rfind(sep, min_break, chunk_size)<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;if break_point != -1:<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return break_point + len(sep)
                    </p>
                </div>

                <p>The 50-character overlap ensures that a question about content at a chunk boundary still finds both neighboring chunks. Without overlap, a sentence split across two chunks becomes invisible to retrieval.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Lazy Evaluation: Don't Re-Embed on Every Query</h3>
                <p>A common RAG mistake is rebuilding the index on every query. The pipeline uses a dirty flag pattern: embeddings and indices are recomputed only when documents change, not when questions are asked.</p>

                <div class="bg-gray-50 rounded-lg p-4 my-4">
                    <p class="text-sm font-mono text-gray-600">
                        def ask(self, question, top_k=5):<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;self._ensure_fitted()  # Only rebuilds if _dirty=True<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;results = self.retriever.search(question, top_k)<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;context = self._build_context(results)  # Max 4,000 chars<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;return self.answer_generator.generate(question, context, results)
                    </p>
                </div>

                <p>The first query after document ingestion pays the cost of fitting the TF-IDF vocabulary and building both indices. Every subsequent query skips straight to retrieval. On a corpus of 100 documents, this reduces query latency from ~800ms (with refitting) to ~15ms (retrieval only).</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Prompt Engineering Lab: A/B Testing for RAG Prompts</h3>
                <p>Different prompt templates produce dramatically different answers from the same retrieved context. "Answer concisely with citations" versus "Provide a detailed analysis with supporting evidence" can change both answer quality and token consumption by 2-3x.</p>

                <p>The system includes 5 built-in prompt templates and an A/B comparison mode. For any question, you can run two templates side-by-side against the same retrieval results and compare the outputs. The cost tracker records token usage per template, so you get hard numbers on the quality-cost tradeoff rather than guessing.</p>

                <div class="bg-gray-50 rounded-lg p-4 my-4">
                    <p class="text-sm font-mono text-gray-600">
                        # Built-in prompt templates<br>
                        qa_concise &nbsp;&nbsp; # Fact lookup: brief answer + citations<br>
                        qa_detailed &nbsp; # Research: thorough analysis + sources<br>
                        summarize &nbsp;&nbsp;&nbsp; # Key points extraction<br>
                        extract_facts  # Structured bullet-point facts<br>
                        compare &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Cross-source comparison
                    </p>
                </div>

                <h3 class="text-xl font-bold mt-8 mb-3">Results</h3>
                <div class="grid grid-cols-3 gap-4 my-6">
                    <div class="bg-green-50 rounded-lg p-4 text-center">
                        <p class="text-2xl font-bold text-green-700">94</p>
                        <p class="text-xs text-green-600 mt-1">Tests passing</p>
                    </div>
                    <div class="bg-green-50 rounded-lg p-4 text-center">
                        <p class="text-2xl font-bold text-green-700">5 formats</p>
                        <p class="text-xs text-green-600 mt-1">PDF, DOCX, TXT, MD, CSV</p>
                    </div>
                    <div class="bg-green-50 rounded-lg p-4 text-center">
                        <p class="text-2xl font-bold text-green-700">0 LLM calls</p>
                        <p class="text-xs text-green-600 mt-1">For ingestion &amp; retrieval</p>
                    </div>
                </div>

                <h3 class="text-xl font-bold mt-8 mb-3">Limitations and Tradeoffs</h3>
                <ul class="list-disc pl-6 space-y-2">
                    <li><strong>TF-IDF is not a true embedding model.</strong> Using TF-IDF cosine similarity as the "dense" index captures more semantic signal than BM25 alone, but less than a transformer-based embedding model like <code>all-MiniLM-L6-v2</code>. The tradeoff is zero external dependencies and fast startup &mdash; no downloading 90MB models on first run.</li>
                    <li><strong>RRF loses score magnitude.</strong> By converting to ranks, you lose how <em>much</em> better the #1 result is versus #2. If one result is overwhelmingly more relevant, RRF treats it the same as a marginal lead. For most document QA workloads this doesn't matter, but for tasks where confidence calibration is important, you'd want a learned fusion model instead.</li>
                    <li><strong>500-character chunks are opinionated.</strong> Legal documents, code files, and scientific papers all have different optimal chunk sizes. The system uses a single chunk size across all documents, which is a simplification. Per-format chunking would improve retrieval quality for mixed corpora.</li>
                    <li><strong>No incremental index updates.</strong> Adding one document rebuilds both indices from scratch. For corpora under 10,000 chunks this completes in under a second. Beyond that, incremental index maintenance (e.g., streaming BM25 updates) would be worth implementing.</li>
                </ul>

                <h3 class="text-xl font-bold mt-8 mb-3">Try It Yourself</h3>
                <p>The full implementation is open source with a <a href="https://ct-document-engine.streamlit.app/" class="text-indigo-600 hover:underline" target="_blank">live demo on Streamlit Cloud</a>. The relevant files:</p>
                <ul class="list-disc pl-6 space-y-1">
                    <li><a href="https://github.com/ChunkyTortoise/docqa-engine/blob/main/src/retriever.py" class="text-indigo-600 hover:underline" target="_blank"><code>src/retriever.py</code></a> &mdash; BM25 index, dense index, and Reciprocal Rank Fusion</li>
                    <li><a href="https://github.com/ChunkyTortoise/docqa-engine/blob/main/src/pipeline.py" class="text-indigo-600 hover:underline" target="_blank"><code>src/pipeline.py</code></a> &mdash; Orchestration, lazy evaluation, context building</li>
                    <li><a href="https://github.com/ChunkyTortoise/docqa-engine/blob/main/src/ingest.py" class="text-indigo-600 hover:underline" target="_blank"><code>src/ingest.py</code></a> &mdash; Multi-format parsing and sentence-aware chunking</li>
                    <li><a href="https://github.com/ChunkyTortoise/docqa-engine/blob/main/src/prompt_lab.py" class="text-indigo-600 hover:underline" target="_blank"><code>src/prompt_lab.py</code></a> &mdash; Template engine and A/B comparison</li>
                </ul>

                <p class="mt-6">For performance benchmarks across all projects, see the <a href="benchmarks.html" class="text-indigo-600 hover:underline">benchmarks page</a>.</p>
            </div>
        </article>

        <!-- Post 1 -->
        <article class="bg-white rounded-xl border p-8 mb-8">
            <div class="flex items-center gap-3 mb-4">
                <span class="bg-indigo-100 text-indigo-700 text-xs px-3 py-1 rounded-full font-medium">LLMOps</span>
                <span class="text-sm text-gray-400">February 2026</span>
            </div>
            <h2 class="text-2xl font-bold mb-6">How I Reduced LLM Token Costs by 89% Without Changing Models</h2>

            <div class="prose prose-gray max-w-none text-gray-700 space-y-4">
                <p>When I started building EnterpriseHub &mdash; a real estate AI platform with 3 specialized chatbots &mdash; each lead qualification workflow consumed 93,000 tokens. At Claude's pricing, that adds up fast. After three rounds of optimization, I got it down to 7,800 tokens per workflow. Here's exactly what I did.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">The Problem: Every Call Sends Everything</h3>
                <p>The naive approach is simple: stuff the full conversation history, system prompt, user profile, and context into every API call. It works. It's also wasteful. Most of that context is irrelevant to the specific question being answered. Worse, the same prompts get re-sent on every interaction &mdash; identical inputs producing identical outputs, billed every time.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Technique 1: Three-Tier Caching (~60% of savings)</h3>
                <p>The single biggest win was caching. Most LLM calls in a business application are repetitive &mdash; the same classification, the same FAQ answer, the same scoring rubric applied to similar inputs.</p>

                <p>I built a three-tier cache:</p>
                <ul class="list-disc pl-6 space-y-2">
                    <li><strong>L1 (In-Memory Dict)</strong> &mdash; Per-request scope. If the same prompt hits the system twice within a single request lifecycle, the second call costs zero tokens and returns in &lt;1ms. This catches the surprisingly common case of redundant calls within a single orchestration flow.</li>
                    <li><strong>L2 (Redis with TTL)</strong> &mdash; Cross-request, shared across all bot instances. Lead qualification questions, market data lookups, and template responses all hit here. TTL tuned per query type: 5 minutes for volatile data, 1 hour for stable templates. Lookup time: ~2ms.</li>
                    <li><strong>L3 (PostgreSQL)</strong> &mdash; Persistent fallback. When Redis is unavailable (restart, network blip), the system degrades gracefully to database-backed cache rather than hitting the API.</li>
                </ul>

                <p>The overall cache hit rate stabilized at 87%. That means 87 out of 100 LLM calls never reach the API at all.</p>

                <div class="bg-gray-50 rounded-lg p-4 my-4">
                    <p class="text-sm font-mono text-gray-600">
                        # From claude_orchestrator.py<br>
                        # In-process cache for memory context (avoids repeated fetches within a request)<br>
                        self._memory_context_cache: Dict[str, Any] = {}<br><br>
                        # Cache check before LLM call<br>
                        cache_key = f"mem_ctx:{lead_id}"<br>
                        cached = self._memory_context_cache.get(cache_key)<br>
                        if cached is not None:<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;memory_context = cached  # Zero tokens, &lt;1ms
                    </p>
                </div>

                <h3 class="text-xl font-bold mt-8 mb-3">Technique 2: Context Window Optimization (~25% of savings)</h3>
                <p>Instead of dumping the full conversation history into every call, I built a sliding window that keeps only what the model actually needs. The <code>ClaudeOrchestrator</code> tracks which conversation turns are relevant to the current task and trims everything else.</p>

                <p>The result: 2.3x more efficient context usage. The model gets the same quality of context in less than half the tokens. This matters especially for long conversations &mdash; a 20-turn qualification flow was sending all 20 turns on every call, when typically only the last 3-5 are relevant.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Technique 3: Model Routing by Task Complexity (~15% of savings)</h3>
                <p>Not every query needs the most capable (and expensive) model. The <code>LLMClient</code> accepts a <code>TaskComplexity</code> parameter that routes requests to the appropriate model:</p>

                <div class="bg-gray-50 rounded-lg p-4 my-4">
                    <p class="text-sm font-mono text-gray-600">
                        class TaskComplexity(Enum):<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;ROUTINE = "routine"      # Simple classification, template fill<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;STANDARD = "standard"    # Default complexity<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;HIGH_STAKES = "high_stakes"  # Complex reasoning, revenue-impacting<br><br>
                        def _get_routed_model(self, complexity):<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;if complexity == TaskComplexity.ROUTINE:<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return self.fast_model    # Cheaper, faster<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;if complexity == TaskComplexity.HIGH_STAKES:<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return self.premium_model  # Full capability
                    </p>
                </div>

                <p>Simple tasks like "Is this a buyer or seller?" go to the fast model. Complex tasks like "Generate a personalized market analysis for this lead" go to the premium model. The router adds &lt;50ms of overhead.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Results</h3>
                <div class="grid grid-cols-3 gap-4 my-6">
                    <div class="bg-green-50 rounded-lg p-4 text-center">
                        <p class="text-2xl font-bold text-green-700">93K &rarr; 7.8K</p>
                        <p class="text-xs text-green-600 mt-1">Tokens per workflow</p>
                    </div>
                    <div class="bg-green-50 rounded-lg p-4 text-center">
                        <p class="text-2xl font-bold text-green-700">87%</p>
                        <p class="text-xs text-green-600 mt-1">Cache hit rate</p>
                    </div>
                    <div class="bg-green-50 rounded-lg p-4 text-center">
                        <p class="text-2xl font-bold text-green-700">&lt;200ms</p>
                        <p class="text-xs text-green-600 mt-1">Orchestrator overhead</p>
                    </div>
                </div>

                <h3 class="text-xl font-bold mt-8 mb-3">Limitations and Tradeoffs</h3>
                <ul class="list-disc pl-6 space-y-2">
                    <li><strong>Cache invalidation is hard.</strong> Stale cached responses are worse than no cache. I use time-based TTLs rather than event-based invalidation because it's simpler and the staleness window is acceptable for my use case.</li>
                    <li><strong>Context windowing can lose information.</strong> The sliding window occasionally drops a relevant early turn. I mitigate this with a summary of dropped context, but it's an imperfect solution.</li>
                    <li><strong>Model routing requires maintenance.</strong> As model pricing and capabilities change, the routing logic needs updating. What was "routine" for one model version may not be for the next.</li>
                    <li><strong>These numbers are specific to my workload.</strong> Lead qualification has high repetition (similar questions, similar leads). Applications with more unique queries will see smaller cache hit rates.</li>
                </ul>

                <h3 class="text-xl font-bold mt-8 mb-3">Try It Yourself</h3>
                <p>The full implementation is open source. The relevant files are:</p>
                <ul class="list-disc pl-6 space-y-1">
                    <li><a href="https://github.com/ChunkyTortoise/EnterpriseHub/blob/main/ghl_real_estate_ai/services/claude_orchestrator.py" class="text-indigo-600 hover:underline" target="_blank"><code>services/claude_orchestrator.py</code></a> &mdash; Cache layers, context management, request orchestration</li>
                    <li><a href="https://github.com/ChunkyTortoise/EnterpriseHub/blob/main/ghl_real_estate_ai/core/llm_client.py" class="text-indigo-600 hover:underline" target="_blank"><code>core/llm_client.py</code></a> &mdash; <code>TaskComplexity</code> routing, model selection, fallback logic</li>
                    <li><a href="https://github.com/ChunkyTortoise/EnterpriseHub/blob/main/ghl_real_estate_ai/services/jorge/performance_tracker.py" class="text-indigo-600 hover:underline" target="_blank"><code>services/jorge/performance_tracker.py</code></a> &mdash; P50/P95/P99 latency tracking, SLA monitoring</li>
                </ul>

                <p class="mt-6">For the full benchmark data, see the <a href="benchmarks.html" class="text-indigo-600 hover:underline">benchmarks page</a>.</p>
            </div>
        </article>
    </section>

    <footer class="bg-gray-900 text-gray-400 py-8">
        <div class="max-w-6xl mx-auto px-4 flex justify-between items-center text-sm">
            <span>&copy; 2026 Cayman Roden</span>
            <div class="flex gap-4">
                <a href="https://github.com/ChunkyTortoise" class="hover:text-white" target="_blank">GitHub</a>
                <a href="https://www.upwork.com/freelancers/~01ee20599d13f4c8c9" class="hover:text-white" target="_blank">Upwork</a>
                <a href="https://linkedin.com/in/caymanroden" class="hover:text-white" target="_blank">LinkedIn</a>
                <a href="mailto:caymanroden@gmail.com" class="hover:text-white">Email</a>
            </div>
        </div>
    </footer>
</body>
</html>
