<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Blog | Cayman Roden</title>
    <meta name="description" content="Technical blog on LLM cost engineering, multi-agent orchestration, and RAG pipeline optimization. Real metrics from production AI systems.">
    <meta property="og:title" content="Blog | Cayman Roden">
    <meta property="og:description" content="Technical blog on LLM cost engineering, multi-agent orchestration, and RAG pipelines. Real metrics from production systems.">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://chunkytortoise.github.io/blog.html">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Blog | Cayman Roden">
    <meta name="twitter:description" content="Technical blog on LLM cost engineering, multi-agent orchestration, and RAG pipelines.">
    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="style.css">
</head>
<body class="bg-gray-50 text-gray-900">
    <nav class="bg-white border-b border-gray-200 sticky top-0 z-50">
        <div class="max-w-6xl mx-auto px-4 py-3 flex justify-between items-center">
            <a href="index.html" class="text-lg font-bold text-indigo-600">Cayman Roden</a>
            <button id="menu-toggle" class="md:hidden p-2 text-gray-600 hover:text-indigo-600" aria-label="Toggle menu">
                <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/></svg>
            </button>
            <div class="hidden md:flex gap-4 text-sm">
                <a href="projects.html" class="hover:text-indigo-600">Projects</a>
                <a href="services.html" class="hover:text-indigo-600">Services</a>
                <a href="certifications.html" class="hover:text-indigo-600">Certifications</a>
                <a href="case-studies.html" class="hover:text-indigo-600">Case Studies</a>
                <a href="benchmarks.html" class="hover:text-indigo-600">Benchmarks</a>
                <a href="blog.html" class="text-indigo-600 font-medium">Blog</a>
                <a href="about.html" class="hover:text-indigo-600">About</a>
            </div>
        </div>
        <div id="mobile-menu" class="hidden md:hidden border-t border-gray-200 bg-white px-4 py-3 space-y-2 text-sm">
            <a href="projects.html" class="block py-1 hover:text-indigo-600">Projects</a>
            <a href="services.html" class="block py-1 hover:text-indigo-600">Services</a>
            <a href="certifications.html" class="block py-1 hover:text-indigo-600">Certifications</a>
            <a href="case-studies.html" class="block py-1 hover:text-indigo-600">Case Studies</a>
            <a href="benchmarks.html" class="block py-1 hover:text-indigo-600">Benchmarks</a>
            <a href="blog.html" class="block py-1 text-indigo-600 font-medium">Blog</a>
            <a href="about.html" class="block py-1 hover:text-indigo-600">About</a>
        </div>
    </nav>
    <script>
    document.getElementById('menu-toggle').addEventListener('click', function() {
        document.getElementById('mobile-menu').classList.toggle('hidden');
    });
    </script>

    <section class="max-w-4xl mx-auto px-4 py-12">
        <h1 class="text-3xl font-bold mb-2">Blog</h1>
        <p class="text-gray-600 mb-10">Technical writeups from building production AI systems.</p>

        <!-- Post 4: Cost Optimization -->
        <article id="post-cost-optimization" class="bg-white rounded-xl border p-8 mb-8">
            <div class="flex items-center gap-3 mb-4">
                <span class="bg-green-100 text-green-700 text-xs px-3 py-1 rounded-full font-medium">Cost Optimization</span>
                <span class="text-sm text-gray-400">February 12, 2026</span>
                <span class="text-sm text-gray-400">&middot; 6 min read</span>
            </div>
            <h2 class="text-2xl font-bold mb-4"><a href="#post-cost-optimization" class="hover:text-indigo-600">How I Cut LLM Costs by 89% Using 3-Tier Caching</a></h2>
            <p class="text-gray-600 mb-4">From 93K to 7.8K tokens per workflow without sacrificing quality. The complete architecture with code samples and real benchmarks from production systems managing 1,000+ daily conversations.</p>
            <a href="#post-cost-optimization" class="text-indigo-600 font-medium hover:underline">Read the full post &rarr;</a>
        </article>

        <!-- Post 3: Multi-Agent -->
        <article class="bg-white rounded-xl border p-8 mb-8">
            <div class="flex items-center gap-3 mb-4">
                <span class="bg-indigo-100 text-indigo-700 text-xs px-3 py-1 rounded-full font-medium">Multi-Agent</span>
                <span class="bg-purple-100 text-purple-700 text-xs px-3 py-1 rounded-full font-medium">Architecture</span>
                <span class="text-sm text-gray-400">February 8, 2026</span>
                <span class="text-sm text-gray-400">&middot; 10 min read</span>
            </div>
            <h2 class="text-2xl font-bold mb-4"><a href="blog/multi-agent-orchestration.html" class="hover:text-indigo-600">Multi-Agent Orchestration: Building Reliable Cross-Bot Handoffs</a></h2>

            <div class="prose prose-gray max-w-none text-gray-700 space-y-4">
                <p>Everyone focuses on making individual AI bots smarter. Better prompts, finer-tuned intent detection, richer context windows. That work matters &mdash; but when you run multiple specialized bots in production, the hardest engineering problem is not inside any single bot. It is the moment one bot decides a conversation belongs to another bot and tries to hand it off mid-flow.</p>

                <p>I built the orchestration layer for <a href="https://github.com/ChunkyTortoise/EnterpriseHub" class="text-indigo-600 hover:underline" target="_blank">EnterpriseHub</a>, a real estate AI platform with three specialized chatbots. This post covers the full orchestration stack: cross-bot handoffs with circular prevention and rate limiting, pattern learning for dynamic threshold adjustment, A/B testing for handoff optimization, and a 7-rule alerting system with 3-level escalation.</p>

                <div class="grid grid-cols-4 gap-4 my-6">
                    <div class="bg-green-50 rounded-lg p-4 text-center">
                        <p class="text-2xl font-bold text-green-700">0</p>
                        <p class="text-xs text-green-600 mt-1">Circular handoffs in production</p>
                    </div>
                    <div class="bg-green-50 rounded-lg p-4 text-center">
                        <p class="text-2xl font-bold text-green-700">4 layers</p>
                        <p class="text-xs text-green-600 mt-1">Safety checks per handoff</p>
                    </div>
                    <div class="bg-green-50 rounded-lg p-4 text-center">
                        <p class="text-2xl font-bold text-green-700">7 rules</p>
                        <p class="text-xs text-green-600 mt-1">Default alert conditions</p>
                    </div>
                    <div class="bg-green-50 rounded-lg p-4 text-center">
                        <p class="text-2xl font-bold text-green-700">3-level</p>
                        <p class="text-xs text-green-600 mt-1">Escalation policy</p>
                    </div>
                </div>

                <p><a href="blog/multi-agent-orchestration.html" class="text-indigo-600 hover:underline font-medium">Read the full post &rarr;</a></p>
            </div>
        </article>

        <!-- Post 2: RAG -->
        <article id="post-rag-pipeline" class="bg-white rounded-xl border p-8 mb-8">
            <div class="flex items-center gap-3 mb-4">
                <span class="bg-blue-100 text-blue-700 text-xs px-3 py-1 rounded-full font-medium">RAG</span>
                <span class="text-sm text-gray-400">February 12, 2026</span>
                <span class="text-sm text-gray-400">&middot; 10 min read</span>
            </div>
            <h2 class="text-2xl font-bold mb-4"><a href="#post-rag-pipeline" class="hover:text-indigo-600">RAG Pipeline Engineering Guide</a></h2>
            <p class="text-gray-600 mb-4">Building document Q&A systems that actually find the right answers. Hybrid retrieval (BM25 + vectors), re-ranking, citation scoring, and query expansion for 87% accuracy on legal document corpora.</p>
            <a href="#post-rag-pipeline" class="text-indigo-600 font-medium hover:underline">Read the full post &rarr;</a>
        </article>

        <!-- Post 1: Multi-Agent Summary -->
        <article id="post-multi-agent-summary" class="bg-white rounded-xl border p-8 mb-8">
            <div class="flex items-center gap-3 mb-4">
                <span class="bg-purple-100 text-purple-700 text-xs px-3 py-1 rounded-full font-medium">Multi-Agent</span>
                <span class="text-sm text-gray-400">February 12, 2026</span>
                <span class="text-sm text-gray-400">&middot; 8 min read</span>
            </div>
            <h2 class="text-2xl font-bold mb-4"><a href="#post-multi-agent-summary" class="hover:text-indigo-600">Building Multi-Agent Systems That Don't Fail</a></h2>
            <p class="text-gray-600 mb-4">How I orchestrated 3 bots with confidence-based handoffs, circular prevention, and 99.7% uptime. Real architecture from a production real estate system processing 1,000+ conversations daily.</p>
            <a href="#post-multi-agent-summary" class="text-indigo-600 font-medium hover:underline">Read the full post &rarr;</a>
        </article>

        <!-- Original Post 2: Hybrid Retrieval -->
        <article class="bg-white rounded-xl border p-8 mb-8">
            <div class="flex items-center gap-3 mb-4">
                <span class="bg-indigo-100 text-indigo-700 text-xs px-3 py-1 rounded-full font-medium">RAG</span>
                <span class="text-sm text-gray-400">February 2026</span>
            </div>
            <h2 class="text-2xl font-bold mb-6">Why Single-Index RAG Fails and How Hybrid Retrieval Fixes It</h2>

            <div class="prose prose-gray max-w-none text-gray-700 space-y-4">
                <p>Most RAG tutorials show a simple pipeline: chunk documents, embed them, do a cosine similarity search, pass the top results to the LLM. It works for demos. It breaks in production. The problem is that a single retrieval method has systematic blind spots &mdash; dense embeddings miss exact keyword matches, and keyword search misses semantic paraphrases. I built a hybrid retrieval system that combines both, and it finds relevant documents that neither method catches alone.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">The Problem: Every Index Has Blind Spots</h3>
                <p>Consider a document about "Section 8 housing voucher programs." A user asks: "What government rental assistance programs are available?" Dense embeddings will match the semantic meaning &mdash; "government rental assistance" is conceptually similar to "housing voucher programs." But if the user asks "Section 8 requirements," a keyword search finds it instantly while the dense index may rank it lower because "Section 8" is a proper noun with no direct semantic relationship to generic terms.</p>

                <p>This is the fundamental tradeoff. Dense retrieval captures <em>meaning</em>. Keyword retrieval captures <em>specifics</em>. Production queries need both. The question is how to combine them without the scores from one method drowning out the other.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Architecture: Dual-Index with Reciprocal Rank Fusion</h3>
                <p>The system maintains two parallel indices over the same document chunks:</p>

                <ul class="list-disc pl-6 space-y-2">
                    <li><strong>BM25 (Okapi)</strong> &mdash; The keyword index. Uses term frequency with saturation (k1=1.5) and document length normalization (b=0.75). IDF is calculated as <code>log((N - df + 0.5) / (df + 0.5) + 1.0)</code> to prevent zero scores on common terms. This catches exact matches, proper nouns, and technical terminology.</li>
                    <li><strong>TF-IDF Dense Vectors</strong> &mdash; The semantic index. Scikit-learn's <code>TfidfVectorizer</code> with a vocabulary cap of 5,000 features generates dense vectors. Cosine similarity with L2 normalization handles the ranking. This catches paraphrases, synonyms, and conceptual similarity.</li>
                </ul>

                <p>Both indices return ranked results for every query. The challenge is combining them. You can't simply average the scores &mdash; BM25 scores and cosine similarities are on completely different scales with different distributions. A BM25 score of 12.7 and a cosine similarity of 0.83 aren't comparable.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Reciprocal Rank Fusion: The Key Insight</h3>
                <p>Instead of comparing raw scores, Reciprocal Rank Fusion (RRF) compares <em>positions</em>. Each result gets a score based on where it appears in each ranked list:</p>

                <div class="bg-gray-50 rounded-lg p-4 my-4">
                    <p class="text-sm font-mono text-gray-600">
                        # Reciprocal Rank Fusion<br>
                        # k=60 balances early-rank sensitivity<br><br>
                        def reciprocal_rank_fusion(ranked_lists, k=60, top_k=5):<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;scores = {}<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;for ranked_list in ranked_lists:<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for rank, (chunk_id, _) in enumerate(ranked_list):<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scores[chunk_id] = scores.get(chunk_id, 0)<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scores[chunk_id] += 1.0 / (k + rank)<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;return sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
                    </p>
                </div>

                <p>The k=60 constant controls how much an early rank matters versus a late rank. A document ranked #1 in one index gets a score of 1/61 = 0.0164. Ranked #10, it gets 1/70 = 0.0143. The falloff is gentle &mdash; being ranked 10th is only slightly worse than 1st. This means a document that ranks well in <em>both</em> indices reliably outscores a document that ranks #1 in one index but doesn't appear in the other.</p>

                <p>One implementation detail that matters: each index retrieves 2x the requested number of results before fusion. If you want the top 5, each index returns 10. This ensures that a document ranked #8 in one index and #3 in the other still gets considered, rather than being cut off.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Chunking: Sentence Boundaries Over Fixed Splits</h3>
                <p>Chunk quality directly determines retrieval quality. I use 500-character chunks with 50-character overlap and sentence-aware boundaries. The algorithm tries to break at sentence-ending periods first, then paragraph breaks, then line breaks, then spaces. It never breaks below half the chunk size to avoid creating fragments.</p>

                <div class="bg-gray-50 rounded-lg p-4 my-4">
                    <p class="text-sm font-mono text-gray-600">
                        # Smart boundary detection (priority order)<br>
                        separators = [". ", "\n\n", "\n", " "]<br>
                        min_break = chunk_size // 2  # 250 chars minimum<br><br>
                        for sep in separators:<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;break_point = text.rfind(sep, min_break, chunk_size)<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;if break_point != -1:<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return break_point + len(sep)
                    </p>
                </div>

                <p>The 50-character overlap ensures that a question about content at a chunk boundary still finds both neighboring chunks. Without overlap, a sentence split across two chunks becomes invisible to retrieval.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Lazy Evaluation: Don't Re-Embed on Every Query</h3>
                <p>A common RAG mistake is rebuilding the index on every query. The pipeline uses a dirty flag pattern: embeddings and indices are recomputed only when documents change, not when questions are asked.</p>

                <div class="bg-gray-50 rounded-lg p-4 my-4">
                    <p class="text-sm font-mono text-gray-600">
                        def ask(self, question, top_k=5):<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;self._ensure_fitted()  # Only rebuilds if _dirty=True<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;results = self.retriever.search(question, top_k)<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;context = self._build_context(results)  # Max 4,000 chars<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;return self.answer_generator.generate(question, context, results)
                    </p>
                </div>

                <p>The first query after document ingestion pays the cost of fitting the TF-IDF vocabulary and building both indices. Every subsequent query skips straight to retrieval. On a corpus of 100 documents, this reduces query latency from ~800ms (with refitting) to ~15ms (retrieval only).</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Prompt Engineering Lab: A/B Testing for RAG Prompts</h3>
                <p>Different prompt templates produce dramatically different answers from the same retrieved context. "Answer concisely with citations" versus "Provide a detailed analysis with supporting evidence" can change both answer quality and token consumption by 2-3x.</p>

                <p>The system includes 5 built-in prompt templates and an A/B comparison mode. For any question, you can run two templates side-by-side against the same retrieval results and compare the outputs. The cost tracker records token usage per template, so you get hard numbers on the quality-cost tradeoff rather than guessing.</p>

                <div class="bg-gray-50 rounded-lg p-4 my-4">
                    <p class="text-sm font-mono text-gray-600">
                        # Built-in prompt templates<br>
                        qa_concise &nbsp;&nbsp; # Fact lookup: brief answer + citations<br>
                        qa_detailed &nbsp; # Research: thorough analysis + sources<br>
                        summarize &nbsp;&nbsp;&nbsp; # Key points extraction<br>
                        extract_facts  # Structured bullet-point facts<br>
                        compare &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Cross-source comparison
                    </p>
                </div>

                <h3 class="text-xl font-bold mt-8 mb-3">Results</h3>
                <div class="grid grid-cols-3 gap-4 my-6">
                    <div class="bg-green-50 rounded-lg p-4 text-center">
                        <p class="text-2xl font-bold text-green-700">94</p>
                        <p class="text-xs text-green-600 mt-1">Tests passing</p>
                    </div>
                    <div class="bg-green-50 rounded-lg p-4 text-center">
                        <p class="text-2xl font-bold text-green-700">5 formats</p>
                        <p class="text-xs text-green-600 mt-1">PDF, DOCX, TXT, MD, CSV</p>
                    </div>
                    <div class="bg-green-50 rounded-lg p-4 text-center">
                        <p class="text-2xl font-bold text-green-700">0 LLM calls</p>
                        <p class="text-xs text-green-600 mt-1">For ingestion &amp; retrieval</p>
                    </div>
                </div>

                <h3 class="text-xl font-bold mt-8 mb-3">Limitations and Tradeoffs</h3>
                <ul class="list-disc pl-6 space-y-2">
                    <li><strong>TF-IDF is not a true embedding model.</strong> Using TF-IDF cosine similarity as the "dense" index captures more semantic signal than BM25 alone, but less than a transformer-based embedding model like <code>all-MiniLM-L6-v2</code>. The tradeoff is zero external dependencies and fast startup &mdash; no downloading 90MB models on first run.</li>
                    <li><strong>RRF loses score magnitude.</strong> By converting to ranks, you lose how <em>much</em> better the #1 result is versus #2. If one result is overwhelmingly more relevant, RRF treats it the same as a marginal lead. For most document QA workloads this doesn't matter, but for tasks where confidence calibration is important, you'd want a learned fusion model instead.</li>
                    <li><strong>500-character chunks are opinionated.</strong> Legal documents, code files, and scientific papers all have different optimal chunk sizes. The system uses a single chunk size across all documents, which is a simplification. Per-format chunking would improve retrieval quality for mixed corpora.</li>
                    <li><strong>No incremental index updates.</strong> Adding one document rebuilds both indices from scratch. For corpora under 10,000 chunks this completes in under a second. Beyond that, incremental index maintenance (e.g., streaming BM25 updates) would be worth implementing.</li>
                </ul>

                <h3 class="text-xl font-bold mt-8 mb-3">Try It Yourself</h3>
                <p>The full implementation is open source with a <a href="https://ct-document-engine.streamlit.app/" class="text-indigo-600 hover:underline" target="_blank">live demo on Streamlit Cloud</a>. The relevant files:</p>
                <ul class="list-disc pl-6 space-y-1">
                    <li><a href="https://github.com/ChunkyTortoise/docqa-engine/blob/main/src/retriever.py" class="text-indigo-600 hover:underline" target="_blank"><code>src/retriever.py</code></a> &mdash; BM25 index, dense index, and Reciprocal Rank Fusion</li>
                    <li><a href="https://github.com/ChunkyTortoise/docqa-engine/blob/main/src/pipeline.py" class="text-indigo-600 hover:underline" target="_blank"><code>src/pipeline.py</code></a> &mdash; Orchestration, lazy evaluation, context building</li>
                    <li><a href="https://github.com/ChunkyTortoise/docqa-engine/blob/main/src/ingest.py" class="text-indigo-600 hover:underline" target="_blank"><code>src/ingest.py</code></a> &mdash; Multi-format parsing and sentence-aware chunking</li>
                    <li><a href="https://github.com/ChunkyTortoise/docqa-engine/blob/main/src/prompt_lab.py" class="text-indigo-600 hover:underline" target="_blank"><code>src/prompt_lab.py</code></a> &mdash; Template engine and A/B comparison</li>
                </ul>

                <p class="mt-6">For performance benchmarks across all projects, see the <a href="benchmarks.html" class="text-indigo-600 hover:underline">benchmarks page</a>.</p>
            </div>
        </article>

        <!-- Post 1 -->
        <article class="bg-white rounded-xl border p-8 mb-8">
            <div class="flex items-center gap-3 mb-4">
                <span class="bg-indigo-100 text-indigo-700 text-xs px-3 py-1 rounded-full font-medium">LLMOps</span>
                <span class="text-sm text-gray-400">February 2026</span>
            </div>
            <h2 class="text-2xl font-bold mb-6">How I Reduced LLM Token Costs by 89% Without Changing Models</h2>

            <div class="prose prose-gray max-w-none text-gray-700 space-y-4">
                <p>When I started building EnterpriseHub &mdash; a real estate AI platform with 3 specialized chatbots &mdash; each lead qualification workflow consumed 93,000 tokens. At Claude's pricing, that adds up fast. After three rounds of optimization, I got it down to 7,800 tokens per workflow. Here's exactly what I did.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">The Problem: Every Call Sends Everything</h3>
                <p>The naive approach is simple: stuff the full conversation history, system prompt, user profile, and context into every API call. It works. It's also wasteful. Most of that context is irrelevant to the specific question being answered. Worse, the same prompts get re-sent on every interaction &mdash; identical inputs producing identical outputs, billed every time.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Technique 1: Three-Tier Caching (~60% of savings)</h3>
                <p>The single biggest win was caching. Most LLM calls in a business application are repetitive &mdash; the same classification, the same FAQ answer, the same scoring rubric applied to similar inputs.</p>

                <p>I built a three-tier cache:</p>
                <ul class="list-disc pl-6 space-y-2">
                    <li><strong>L1 (In-Memory Dict)</strong> &mdash; Per-request scope. If the same prompt hits the system twice within a single request lifecycle, the second call costs zero tokens and returns in &lt;1ms. This catches the surprisingly common case of redundant calls within a single orchestration flow.</li>
                    <li><strong>L2 (Redis with TTL)</strong> &mdash; Cross-request, shared across all bot instances. Lead qualification questions, market data lookups, and template responses all hit here. TTL tuned per query type: 5 minutes for volatile data, 1 hour for stable templates. Lookup time: ~2ms.</li>
                    <li><strong>L3 (PostgreSQL)</strong> &mdash; Persistent fallback. When Redis is unavailable (restart, network blip), the system degrades gracefully to database-backed cache rather than hitting the API.</li>
                </ul>

                <p>The overall cache hit rate stabilized at 87%. That means 87 out of 100 LLM calls never reach the API at all.</p>

                <div class="bg-gray-50 rounded-lg p-4 my-4">
                    <p class="text-sm font-mono text-gray-600">
                        # From claude_orchestrator.py<br>
                        # In-process cache for memory context (avoids repeated fetches within a request)<br>
                        self._memory_context_cache: Dict[str, Any] = {}<br><br>
                        # Cache check before LLM call<br>
                        cache_key = f"mem_ctx:{lead_id}"<br>
                        cached = self._memory_context_cache.get(cache_key)<br>
                        if cached is not None:<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;memory_context = cached  # Zero tokens, &lt;1ms
                    </p>
                </div>

                <h3 class="text-xl font-bold mt-8 mb-3">Technique 2: Context Window Optimization (~25% of savings)</h3>
                <p>Instead of dumping the full conversation history into every call, I built a sliding window that keeps only what the model actually needs. The <code>ClaudeOrchestrator</code> tracks which conversation turns are relevant to the current task and trims everything else.</p>

                <p>The result: 2.3x more efficient context usage. The model gets the same quality of context in less than half the tokens. This matters especially for long conversations &mdash; a 20-turn qualification flow was sending all 20 turns on every call, when typically only the last 3-5 are relevant.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Technique 3: Model Routing by Task Complexity (~15% of savings)</h3>
                <p>Not every query needs the most capable (and expensive) model. The <code>LLMClient</code> accepts a <code>TaskComplexity</code> parameter that routes requests to the appropriate model:</p>

                <div class="bg-gray-50 rounded-lg p-4 my-4">
                    <p class="text-sm font-mono text-gray-600">
                        class TaskComplexity(Enum):<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;ROUTINE = "routine"      # Simple classification, template fill<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;STANDARD = "standard"    # Default complexity<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;HIGH_STAKES = "high_stakes"  # Complex reasoning, revenue-impacting<br><br>
                        def _get_routed_model(self, complexity):<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;if complexity == TaskComplexity.ROUTINE:<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return self.fast_model    # Cheaper, faster<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;if complexity == TaskComplexity.HIGH_STAKES:<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return self.premium_model  # Full capability
                    </p>
                </div>

                <p>Simple tasks like "Is this a buyer or seller?" go to the fast model. Complex tasks like "Generate a personalized market analysis for this lead" go to the premium model. The router adds &lt;50ms of overhead.</p>

                <h3 class="text-xl font-bold mt-8 mb-3">Results</h3>
                <div class="grid grid-cols-3 gap-4 my-6">
                    <div class="bg-green-50 rounded-lg p-4 text-center">
                        <p class="text-2xl font-bold text-green-700">93K &rarr; 7.8K</p>
                        <p class="text-xs text-green-600 mt-1">Tokens per workflow</p>
                    </div>
                    <div class="bg-green-50 rounded-lg p-4 text-center">
                        <p class="text-2xl font-bold text-green-700">87%</p>
                        <p class="text-xs text-green-600 mt-1">Cache hit rate</p>
                    </div>
                    <div class="bg-green-50 rounded-lg p-4 text-center">
                        <p class="text-2xl font-bold text-green-700">&lt;200ms</p>
                        <p class="text-xs text-green-600 mt-1">Orchestrator overhead</p>
                    </div>
                </div>

                <h3 class="text-xl font-bold mt-8 mb-3">Limitations and Tradeoffs</h3>
                <ul class="list-disc pl-6 space-y-2">
                    <li><strong>Cache invalidation is hard.</strong> Stale cached responses are worse than no cache. I use time-based TTLs rather than event-based invalidation because it's simpler and the staleness window is acceptable for my use case.</li>
                    <li><strong>Context windowing can lose information.</strong> The sliding window occasionally drops a relevant early turn. I mitigate this with a summary of dropped context, but it's an imperfect solution.</li>
                    <li><strong>Model routing requires maintenance.</strong> As model pricing and capabilities change, the routing logic needs updating. What was "routine" for one model version may not be for the next.</li>
                    <li><strong>These numbers are specific to my workload.</strong> Lead qualification has high repetition (similar questions, similar leads). Applications with more unique queries will see smaller cache hit rates.</li>
                </ul>

                <h3 class="text-xl font-bold mt-8 mb-3">Try It Yourself</h3>
                <p>The full implementation is open source. The relevant files are:</p>
                <ul class="list-disc pl-6 space-y-1">
                    <li><a href="https://github.com/ChunkyTortoise/EnterpriseHub/blob/main/ghl_real_estate_ai/services/claude_orchestrator.py" class="text-indigo-600 hover:underline" target="_blank"><code>services/claude_orchestrator.py</code></a> &mdash; Cache layers, context management, request orchestration</li>
                    <li><a href="https://github.com/ChunkyTortoise/EnterpriseHub/blob/main/ghl_real_estate_ai/core/llm_client.py" class="text-indigo-600 hover:underline" target="_blank"><code>core/llm_client.py</code></a> &mdash; <code>TaskComplexity</code> routing, model selection, fallback logic</li>
                    <li><a href="https://github.com/ChunkyTortoise/EnterpriseHub/blob/main/ghl_real_estate_ai/services/jorge/performance_tracker.py" class="text-indigo-600 hover:underline" target="_blank"><code>services/jorge/performance_tracker.py</code></a> &mdash; P50/P95/P99 latency tracking, SLA monitoring</li>
                </ul>

                <p class="mt-6">For the full benchmark data, see the <a href="benchmarks.html" class="text-indigo-600 hover:underline">benchmarks page</a>.</p>
            </div>
        </article>
    </section>

    <footer class="bg-gray-900 text-gray-400 py-8">
        <div class="max-w-6xl mx-auto px-4 flex justify-between items-center text-sm">
            <span>&copy; 2026 Cayman Roden</span>
            <div class="flex gap-4">
                <a href="https://github.com/ChunkyTortoise" class="hover:text-white" target="_blank">GitHub</a>
                <a href="https://www.upwork.com/freelancers/~01ee20599d13f4c8c9" class="hover:text-white" target="_blank">Upwork</a>
                <a href="https://linkedin.com/in/cayman-roden" class="hover:text-white" target="_blank">LinkedIn</a>
                <a href="mailto:caymanroden@gmail.com" class="hover:text-white">Email</a>
            </div>
        </div>
    </footer>
</body>
</html>
